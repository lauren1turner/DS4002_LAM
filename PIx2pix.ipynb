{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsGz6YoyiWu3Nf6ucK7ChR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauren1turner/DS4002_LAM/blob/project3/PIx2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhpGei_pUVab",
        "outputId": "ce19a144-34e1-4865-974f-3436bf1b2361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders created successfully:\n",
            "/content/dataset/train/grayscale\n",
            "/content/dataset/train/color\n",
            "/content/dataset/test/grayscale\n",
            "/content/dataset/test/color\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "base_dir = \"/content/dataset\"\n",
        "\n",
        "folders = [\n",
        "    \"train/grayscale\",\n",
        "    \"train/color\",\n",
        "    \"test/grayscale\",\n",
        "    \"test/color\"\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    path = os.path.join(base_dir, folder)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "print(\"Folders created successfully:\")\n",
        "for folder in folders:\n",
        "    print(os.path.join(base_dir, folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Set the folder path (where your .jpg images are located)\n",
        "folder_path = \"/content/\"  # Replace with your actual image path\n",
        "output_folder = \"/content/dataset/test/grayscale\"  # Folder to save grayscaled images\n",
        "\n",
        "# Ensure the output folder exists\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Loop through each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.lower().endswith(\".jpg\"):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Open the image and convert it to grayscale\n",
        "        img = Image.open(image_path).convert(\"L\")\n",
        "\n",
        "        # Modify filename to add .grayscale before .jpg\n",
        "        name, ext = os.path.splitext(filename)\n",
        "        new_filename = f\"{name}.grayscale.jpg\"\n",
        "\n",
        "        # Save the grayscale image\n",
        "        save_path = os.path.join(output_folder, new_filename)\n",
        "        img.save(save_path)\n",
        "\n",
        "        print(f\"Grayscaled: {new_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnR9Wqr3VBB_",
        "outputId": "f7cae222-5b07-4e72-bad2-88d85344f632"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grayscaled: image-202-c.grayscale.jpg\n",
            "Grayscaled: image-277-a.grayscale.jpg\n",
            "Grayscaled: image-223-c.grayscale.jpg\n",
            "Grayscaled: image-214-c.grayscale.jpg\n",
            "Grayscaled: image-234-a.grayscale.jpg\n",
            "Grayscaled: image-288-a.grayscale.jpg\n",
            "Grayscaled: image-270-a.grayscale.jpg\n",
            "Grayscaled: image-296-a.grayscale.jpg\n",
            "Grayscaled: image-291-a.grayscale.jpg\n",
            "Grayscaled: image-205-c.grayscale.jpg\n",
            "Grayscaled: image-189-a.grayscale.jpg\n",
            "Grayscaled: image-171-a.grayscale.jpg\n",
            "Grayscaled: image-190-a.grayscale.jpg\n",
            "Grayscaled: image-174-a.grayscale.jpg\n",
            "Grayscaled: image-280-a.grayscale.jpg\n",
            "Grayscaled: image-298-a.grayscale.jpg\n",
            "Grayscaled: image-286-a.grayscale.jpg\n",
            "Grayscaled: image-282-a.grayscale.jpg\n",
            "Grayscaled: image-213-a.grayscale.jpg\n",
            "Grayscaled: image-297-a.grayscale.jpg\n",
            "Grayscaled: image-196-a.grayscale.jpg\n",
            "Grayscaled: image-188-a.grayscale.jpg\n",
            "Grayscaled: image-179-a.grayscale.jpg\n",
            "Grayscaled: image-184-a.grayscale.jpg\n",
            "Grayscaled: image-225-a.grayscale.jpg\n",
            "Grayscaled: image-211-c.grayscale.jpg\n",
            "Grayscaled: image-187-a.grayscale.jpg\n",
            "Grayscaled: image-229-c.grayscale.jpg\n",
            "Grayscaled: image-207-a.grayscale.jpg\n",
            "Grayscaled: image-284-a.grayscale.jpg\n",
            "Grayscaled: image-177-a.grayscale.jpg\n",
            "Grayscaled: image-173-a.grayscale.jpg\n",
            "Grayscaled: image-198-a.grayscale.jpg\n",
            "Grayscaled: image-216-a.grayscale.jpg\n",
            "Grayscaled: image-220-c.grayscale.jpg\n",
            "Grayscaled: image-210-a.grayscale.jpg\n",
            "Grayscaled: image-285-a.grayscale.jpg\n",
            "Grayscaled: image-186-a.grayscale.jpg\n",
            "Grayscaled: image-274-a.grayscale.jpg\n",
            "Grayscaled: image-278-a.grayscale.jpg\n",
            "Grayscaled: image-183-a.grayscale.jpg\n",
            "Grayscaled: image-176-a.grayscale.jpg\n",
            "Grayscaled: image-294-a.grayscale.jpg\n",
            "Grayscaled: image-287-a.grayscale.jpg\n",
            "Grayscaled: image-290-a.grayscale.jpg\n",
            "Grayscaled: image-175-a.grayscale.jpg\n",
            "Grayscaled: image-235-c.grayscale.jpg\n",
            "Grayscaled: image-289-a.grayscale.jpg\n",
            "Grayscaled: image-271-a.grayscale.jpg\n",
            "Grayscaled: image-273-a.grayscale.jpg\n",
            "Grayscaled: image-217-c.grayscale.jpg\n",
            "Grayscaled: image-299-a.grayscale.jpg\n",
            "Grayscaled: image-208-c.grayscale.jpg\n",
            "Grayscaled: image-181-a.grayscale.jpg\n",
            "Grayscaled: image-204-a.grayscale.jpg\n",
            "Grayscaled: image-279-a.grayscale.jpg\n",
            "Grayscaled: image-283-a.grayscale.jpg\n",
            "Grayscaled: image-195-a.grayscale.jpg\n",
            "Grayscaled: image-281-a.grayscale.jpg\n",
            "Grayscaled: image-180-a.grayscale.jpg\n",
            "Grayscaled: image-272-a.grayscale.jpg\n",
            "Grayscaled: image-276-a.grayscale.jpg\n",
            "Grayscaled: image-293-a.grayscale.jpg\n",
            "Grayscaled: image-193-a.grayscale.jpg\n",
            "Grayscaled: image-192-a.grayscale.jpg\n",
            "Grayscaled: image-222-a.grayscale.jpg\n",
            "Grayscaled: image-292-a.grayscale.jpg\n",
            "Grayscaled: image-219-a.grayscale.jpg\n",
            "Grayscaled: image-170-a.grayscale.jpg\n",
            "Grayscaled: image-178-a.grayscale.jpg\n",
            "Grayscaled: image-295-a.grayscale.jpg\n",
            "Grayscaled: image-185-a.grayscale.jpg\n",
            "Grayscaled: image-182-a.grayscale.jpg\n",
            "Grayscaled: image-194-a.grayscale.jpg\n",
            "Grayscaled: image-231-a.grayscale.jpg\n",
            "Grayscaled: image-232-c.grayscale.jpg\n",
            "Grayscaled: image-275-a.grayscale.jpg\n",
            "Grayscaled: image-226-c.grayscale.jpg\n",
            "Grayscaled: image-228-a.grayscale.jpg\n",
            "Grayscaled: image-197-a.grayscale.jpg\n",
            "Grayscaled: image-201-a.grayscale.jpg\n",
            "Grayscaled: image-172-a.grayscale.jpg\n",
            "Grayscaled: image-199-a.grayscale.jpg\n",
            "Grayscaled: image-191-a.grayscale.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir = \"/content\"\n",
        "target_dir = \"/content/dataset/test/color\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Move all .jpg files\n",
        "for filename in os.listdir(source_dir):\n",
        "    if filename.lower().endswith(\".jpg\"):\n",
        "        shutil.move(os.path.join(source_dir, filename), os.path.join(target_dir, filename))\n",
        "\n",
        "print(\"✅ All .jpg files moved to /content/dataset/test/color\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht5V_fD3Wdiy",
        "outputId": "ccacfe92-a4da-4f36-fa02-5cdad1821782"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All .jpg files moved to /content/dataset/test/color\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision matplotlib\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# === Dataset ===\n",
        "class GrayscaleToColorDataset(Dataset):\n",
        "    def __init__(self, grayscale_dir, color_dir, transform=None):\n",
        "        self.grayscale_dir = grayscale_dir\n",
        "        self.color_dir = color_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(grayscale_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gray_path = os.path.join(self.grayscale_dir, self.image_names[idx])\n",
        "        color_path = os.path.join(self.color_dir, self.image_names[idx])\n",
        "\n",
        "        gray_image = Image.open(gray_path).convert('L')\n",
        "        color_image = Image.open(color_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            gray_image = self.transform(gray_image)\n",
        "            color_image = self.transform(color_image)\n",
        "\n",
        "        return gray_image, color_image\n",
        "\n",
        "# === Transforms ===\n",
        "img_size = 256\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = GrayscaleToColorDataset(\n",
        "    grayscale_dir=\"/content/dataset/train/grayscale\",\n",
        "    color_dir=\"/content/dataset/train/color\",\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# === Generator ===\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, out_channels, 4, 2, 1), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# === Discriminator ===\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 1, 4, 1, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Concatenate input and target image\n",
        "        return self.net(torch.cat([x, y], 1))\n",
        "\n",
        "# === Training ===\n",
        "generator = UNetGenerator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=2e-4)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4)\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (gray, color) in enumerate(train_loader):\n",
        "        gray, color = gray.to(device), color.to(device)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        fake_color = generator(gray)\n",
        "        real_label = torch.ones((gray.size(0), 1, 30, 30), device=device)\n",
        "        fake_label = torch.zeros((gray.size(0), 1, 30, 30), device=device)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        real_output = discriminator(gray, color)\n",
        "        fake_output = discriminator(gray, fake_color.detach())\n",
        "        d_loss = (criterion(real_output, real_label) + criterion(fake_output, fake_label)) * 0.5\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_output = discriminator(gray, fake_color)\n",
        "        g_loss = criterion(fake_output, real_label) + l1_loss(fake_color, color) * 100\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_image(fake_color, f\"/content/fake_epoch_{epoch+1}.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "tWqBpruwU7NC",
        "outputId": "f35a29dc-88ad-4046-e7e2-95bac4167340"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dataset/train/color/image-292-a.grayscale.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e348b141b39e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e348b141b39e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mgray_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mcolor_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3465\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3466\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset/train/color/image-292-a.grayscale.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "paths = [\n",
        "    (\"/content/dataset/test/color\", \"/content/dataset/train/color\"),\n",
        "    (\"/content/dataset/test/grayscale\", \"/content/dataset/train/grayscale\")\n",
        "]\n",
        "\n",
        "# Copy files\n",
        "for src, dst in paths:\n",
        "    os.makedirs(dst, exist_ok=True)  # Make sure the target folder exists\n",
        "    for filename in os.listdir(src):\n",
        "        src_file = os.path.join(src, filename)\n",
        "        dst_file = os.path.join(dst, filename)\n",
        "        if os.path.isfile(src_file):\n",
        "            shutil.copy(src_file, dst_file)\n",
        "\n",
        "print(\"✅ Files copied from test to train folders.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvO_dh-0WmxM",
        "outputId": "18150228-087b-48c9-c7f6-35798f258859"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Files copied from test to train folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Color:\", len(os.listdir('/content/dataset/train/color')))\n",
        "print(\"Train Grayscale:\", len(os.listdir('/content/dataset/train/grayscale')))\n",
        "print(\"Test Color:\", len(os.listdir('/content/dataset/test/color')))\n",
        "print(\"Test Grayscale:\", len(os.listdir('/content/dataset/test/grayscale')))\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "color_files = os.listdir(\"/content/dataset/train/color\")\n",
        "print(\"Sample color files:\", color_files[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlGNipaHYGW3",
        "outputId": "4937d145-0b60-4238-886e-3a08e8a33570"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Color: 84\n",
            "Train Grayscale: 84\n",
            "Test Color: 84\n",
            "Test Grayscale: 84\n",
            "Sample color files: ['image-202-c.jpg', 'image-277-a.jpg', 'image-223-c.jpg', 'image-214-c.jpg', 'image-234-a.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GrayscaleToColorDataset(Dataset):\n",
        "    def __init__(self, grayscale_dir, color_dir, transform=None):\n",
        "        self.grayscale_dir = grayscale_dir\n",
        "        self.color_dir = color_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.grayscale_files = os.listdir(grayscale_dir)\n",
        "        self.color_files = os.listdir(color_dir)\n",
        "\n",
        "        # Normalize filenames (remove .grayscale, lowercased)\n",
        "        def normalize(name):\n",
        "            return name.replace(\".grayscale\", \"\").lower()\n",
        "\n",
        "        # Build matched pairs\n",
        "        self.pairs = []\n",
        "        color_map = {normalize(f): f for f in self.color_files}\n",
        "        for gray in self.grayscale_files:\n",
        "            norm = normalize(gray)\n",
        "            if norm in color_map:\n",
        "                self.pairs.append((gray, color_map[norm]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gray_name, color_name = self.pairs[idx]\n",
        "        gray_path = os.path.join(self.grayscale_dir, gray_name)\n",
        "        color_path = os.path.join(self.color_dir, color_name)\n",
        "\n",
        "        gray_image = Image.open(gray_path).convert('L')\n",
        "        color_image = Image.open(color_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            gray_image = self.transform(gray_image)\n",
        "            color_image = self.transform(color_image)\n",
        "\n",
        "        return gray_image, color_image\n"
      ],
      "metadata": {
        "id": "EfOEkv44Y1Ws"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "\n",
        "# === Device ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# === Dataset with Automatic Matching ===\n",
        "class GrayscaleToColorDataset(Dataset):\n",
        "    def __init__(self, grayscale_dir, color_dir, transform=None):\n",
        "        self.grayscale_dir = grayscale_dir\n",
        "        self.color_dir = color_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.grayscale_files = os.listdir(grayscale_dir)\n",
        "        self.color_files = os.listdir(color_dir)\n",
        "\n",
        "        def normalize(name):\n",
        "            return name.replace(\".grayscale\", \"\").lower()\n",
        "\n",
        "        # Build matched grayscale-color pairs\n",
        "        color_map = {normalize(f): f for f in self.color_files}\n",
        "        self.pairs = []\n",
        "        for gray in self.grayscale_files:\n",
        "            norm = normalize(gray)\n",
        "            if norm in color_map:\n",
        "                self.pairs.append((gray, color_map[norm]))\n",
        "\n",
        "        print(f\"✅ Matched {len(self.pairs)} grayscale-color pairs.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gray_name, color_name = self.pairs[idx]\n",
        "        gray_path = os.path.join(self.grayscale_dir, gray_name)\n",
        "        color_path = os.path.join(self.color_dir, color_name)\n",
        "\n",
        "        gray_image = Image.open(gray_path).convert('L')\n",
        "        color_image = Image.open(color_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            gray_image = self.transform(gray_image)\n",
        "            color_image = self.transform(color_image)\n",
        "\n",
        "        return gray_image, color_image\n",
        "\n",
        "# === Transforms ===\n",
        "img_size = 256\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# === DataLoader ===\n",
        "train_dataset = GrayscaleToColorDataset(\n",
        "    grayscale_dir=\"/content/dataset/train/grayscale\",\n",
        "    color_dir=\"/content/dataset/train/color\",\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# === Generator (U-Net) ===\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, out_channels, 4, 2, 1), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# === Discriminator (PatchGAN) ===\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 1, 4, 1, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.net(torch.cat([x, y], 1))  # Concatenate grayscale and color images\n",
        "\n",
        "# === Model Init ===\n",
        "generator = UNetGenerator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# === Loss and Optimizers ===\n",
        "criterion = nn.BCELoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=2e-4)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4)\n",
        "\n",
        "# === Training Loop ===\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    for i, (gray, color) in enumerate(train_loader):\n",
        "        gray, color = gray.to(device), color.to(device)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        fake_color = generator(gray)\n",
        "        real_label = torch.ones((gray.size(0), 1, 30, 30), device=device)\n",
        "        fake_label = torch.zeros((gray.size(0), 1, 30, 30), device=device)\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        real_output = discriminator(gray, color)\n",
        "        fake_output = discriminator(gray, fake_color.detach())\n",
        "        d_loss = (criterion(real_output, real_label) + criterion(fake_output, fake_label)) * 0.5\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_output = discriminator(gray, fake_color)\n",
        "        g_loss = criterion(fake_output, real_label) + l1_loss(fake_color, color) * 100\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # === Save sample output ===\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_image(fake_color, f\"/content/fake_epoch_{epoch+1}.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "wSuH6-uLaaKO",
        "outputId": "afc9b5ab-af35-40b0-ae02-f64e40c8108b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "✅ Matched 84 grayscale-color pairs.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Using a target size (torch.Size([4, 1, 30, 30])) that is different to the input size (torch.Size([4, 1, 63, 63])) is deprecated. Please ensure they have the same size.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4fd594e20a40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mreal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_color\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         return F.binary_cross_entropy(\n\u001b[0m\u001b[1;32m    700\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3558\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3560\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3561\u001b[0m             \u001b[0;34mf\"Using a target size ({target.size()}) that is different to the input size ({input.size()}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3562\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([4, 1, 30, 30])) that is different to the input size (torch.Size([4, 1, 63, 63])) is deprecated. Please ensure they have the same size."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GrayscaleToColorDataset(Dataset):\n",
        "    def __init__(self, grayscale_dir, color_dir, transform=None, target_size=(256, 256)):\n",
        "        self.grayscale_dir = grayscale_dir\n",
        "        self.color_dir = color_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "\n",
        "        self.grayscale_files = os.listdir(grayscale_dir)\n",
        "        self.color_files = os.listdir(color_dir)\n",
        "\n",
        "        # Normalize filenames\n",
        "        def normalize(name):\n",
        "            return name.replace(\".grayscale\", \"\").lower()\n",
        "\n",
        "        # Build matched grayscale-color pairs\n",
        "        color_map = {normalize(f): f for f in self.color_files}\n",
        "        self.pairs = []\n",
        "        for gray in self.grayscale_files:\n",
        "            norm = normalize(gray)\n",
        "            if norm in color_map:\n",
        "                gray_path = os.path.join(self.grayscale_dir, gray)\n",
        "                color_path = os.path.join(self.color_dir, color_map[norm])\n",
        "\n",
        "                # Check sizes\n",
        "                gray_image = Image.open(gray_path)\n",
        "                color_image = Image.open(color_path)\n",
        "\n",
        "                if gray_image.size == color_image.size == self.target_size:\n",
        "                    self.pairs.append((gray, color_map[norm]))\n",
        "\n",
        "        print(f\"✅ Matched {len(self.pairs)} grayscale-color pairs with target size {self.target_size}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gray_name, color_name = self.pairs[idx]\n",
        "        gray_path = os.path.join(self.grayscale_dir, gray_name)\n",
        "        color_path = os.path.join(self.color_dir, color_name)\n",
        "\n",
        "        gray_image = Image.open(gray_path).convert('L')\n",
        "        color_image = Image.open(color_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            gray_image = self.transform(gray_image)\n",
        "            color_image = self.transform(color_image)\n",
        "\n",
        "        return gray_image, color_image\n"
      ],
      "metadata": {
        "id": "PWnA5Z_fap-N"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# Directories for grayscale and color images\n",
        "grayscale_dir = \"/content/dataset/train/grayscale\"\n",
        "color_dir = \"/content/dataset/train/color\"\n",
        "target_size = (1024, 768)  # Target size for filtering\n",
        "\n",
        "# Function to filter images\n",
        "def filter_images(input_dir, output_dir, target_size):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        image_path = os.path.join(input_dir, filename)\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Check if the image size matches the target size\n",
        "        if image.size == target_size:\n",
        "            # If size matches, copy the image to the output directory\n",
        "            shutil.copy(image_path, os.path.join(output_dir, filename))\n",
        "        else:\n",
        "            print(f\"Skipping {filename} (size {image.size})\")\n",
        "\n",
        "# Filter grayscale images\n",
        "filter_images(grayscale_dir, \"/content/dataset/train/grayscale_filtered\", target_size)\n",
        "\n",
        "# Filter color images\n",
        "filter_images(color_dir, \"/content/dataset/train/color_filtered\", target_size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn3GlAnVa0iv",
        "outputId": "48d43693-a5d4-4044-9d45-87e874f38ab7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping image-298-a.grayscale.jpg (size (1024, 683))\n",
            "Skipping image-271-a.grayscale.jpg (size (790, 768))\n",
            "Skipping image-288-a.grayscale.jpg (size (890, 768))\n",
            "Skipping image-279-a.grayscale.jpg (size (1024, 683))\n",
            "Skipping image-290-a.grayscale.jpg (size (817, 768))\n",
            "Skipping image-270-a.grayscale.jpg (size (1024, 608))\n",
            "Skipping image-273-a.grayscale.jpg (size (1024, 568))\n",
            "Skipping image-278-a.grayscale.jpg (size (1024, 729))\n",
            "Skipping image-288-a.jpg (size (890, 768))\n",
            "Skipping image-270-a.jpg (size (1024, 608))\n",
            "Skipping image-298-a.jpg (size (1024, 683))\n",
            "Skipping image-278-a.jpg (size (1024, 729))\n",
            "Skipping image-290-a.jpg (size (817, 768))\n",
            "Skipping image-271-a.jpg (size (790, 768))\n",
            "Skipping image-273-a.jpg (size (1024, 568))\n",
            "Skipping image-279-a.jpg (size (1024, 683))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# Directories for test grayscale and color images\n",
        "grayscale_test_dir = \"/content/dataset/test/grayscale\"\n",
        "color_test_dir = \"/content/dataset/test/color\"\n",
        "target_size = (1024, 768)  # Target size for filtering\n",
        "\n",
        "# Function to filter images\n",
        "def filter_images(input_dir, output_dir, target_size):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        image_path = os.path.join(input_dir, filename)\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Check if the image size matches the target size\n",
        "        if image.size == target_size:\n",
        "            # If size matches, copy the image to the output directory\n",
        "            shutil.copy(image_path, os.path.join(output_dir, filename))\n",
        "        else:\n",
        "            print(f\"Skipping {filename} (size {image.size})\")\n",
        "\n",
        "# Filter test grayscale images\n",
        "filter_images(grayscale_test_dir, \"/content/dataset/test/grayscale_filtered\", target_size)\n",
        "\n",
        "# Filter test color images\n",
        "filter_images(color_test_dir, \"/content/dataset/test/color_filtered\", target_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7vncJF9a067",
        "outputId": "161c3272-9e52-4d06-9a8b-c9c2433f5d56"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping image-298-a.grayscale.jpg (size (1024, 683))\n",
            "Skipping image-271-a.grayscale.jpg (size (790, 768))\n",
            "Skipping image-288-a.grayscale.jpg (size (890, 768))\n",
            "Skipping image-279-a.grayscale.jpg (size (1024, 683))\n",
            "Skipping image-290-a.grayscale.jpg (size (817, 768))\n",
            "Skipping image-270-a.grayscale.jpg (size (1024, 608))\n",
            "Skipping image-273-a.grayscale.jpg (size (1024, 568))\n",
            "Skipping image-278-a.grayscale.jpg (size (1024, 729))\n",
            "Skipping image-288-a.jpg (size (890, 768))\n",
            "Skipping image-270-a.jpg (size (1024, 608))\n",
            "Skipping image-298-a.jpg (size (1024, 683))\n",
            "Skipping image-278-a.jpg (size (1024, 729))\n",
            "Skipping image-290-a.jpg (size (817, 768))\n",
            "Skipping image-271-a.jpg (size (790, 768))\n",
            "Skipping image-273-a.jpg (size (1024, 568))\n",
            "Skipping image-279-a.jpg (size (1024, 683))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "# === Device ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# === Dataset with Automatic Matching ===\n",
        "class GrayscaleToColorDataset(Dataset):\n",
        "    def __init__(self, grayscale_dir, color_dir, transform=None):\n",
        "        self.grayscale_dir = grayscale_dir\n",
        "        self.color_dir = color_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.grayscale_files = os.listdir(grayscale_dir)\n",
        "        self.color_files = os.listdir(color_dir)\n",
        "\n",
        "        def normalize(name):\n",
        "            return name.replace(\".grayscale\", \"\").lower()\n",
        "\n",
        "        # Build matched grayscale-color pairs\n",
        "        color_map = {normalize(f): f for f in self.color_files}\n",
        "        self.pairs = []\n",
        "        for gray in self.grayscale_files:\n",
        "            norm = normalize(gray)\n",
        "            if norm in color_map:\n",
        "                self.pairs.append((gray, color_map[norm]))\n",
        "\n",
        "        print(f\"✅ Matched {len(self.pairs)} grayscale-color pairs.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gray_name, color_name = self.pairs[idx]\n",
        "        gray_path = os.path.join(self.grayscale_dir, gray_name)\n",
        "        color_path = os.path.join(self.color_dir, color_name)\n",
        "\n",
        "        gray_image = Image.open(gray_path).convert('L')\n",
        "        color_image = Image.open(color_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            gray_image = self.transform(gray_image)\n",
        "            color_image = self.transform(color_image)\n",
        "\n",
        "        return gray_image, color_image\n",
        "\n",
        "# === Transforms ===\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()  # Only convert images to tensors, no resizing\n",
        "])\n",
        "\n",
        "# === DataLoader ===\n",
        "train_dataset = GrayscaleToColorDataset(\n",
        "    grayscale_dir=\"/content/dataset/train/grayscale_filtered\",\n",
        "    color_dir=\"/content/dataset/train/color_filtered\",\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# === Generator (U-Net) ===\n",
        "class UNetGenerator(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=3):\n",
        "        super(UNetGenerator, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, out_channels, 4, 2, 1), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# === Discriminator (PatchGAN) ===\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, 4, 2, 1), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(128, 1, 4, 1, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return self.net(torch.cat([x, y], 1))  # Concatenate grayscale and color images\n",
        "\n",
        "# === Model Init ===\n",
        "generator = UNetGenerator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# === Loss and Optimizers ===\n",
        "criterion = nn.BCELoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=2e-4)\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4)\n",
        "\n",
        "# === Training Loop ===\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    for i, (gray, color) in enumerate(train_loader):\n",
        "        gray, color = gray.to(device), color.to(device)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        fake_color = generator(gray)\n",
        "\n",
        "        # Calculate the output size of the discriminator (patch size)\n",
        "        output_size = discriminator(gray, color).size()[2:]  # (H, W)\n",
        "\n",
        "        # Resize the labels to match the output size of the discriminator\n",
        "        real_label = torch.ones((gray.size(0), 1) + output_size, device=device)\n",
        "        fake_label = torch.zeros((gray.size(0), 1) + output_size, device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer_D.zero_grad()\n",
        "        real_output = discriminator(gray, color)\n",
        "        fake_output = discriminator(gray, fake_color.detach())\n",
        "\n",
        "        # Compute the loss\n",
        "        d_loss = (criterion(real_output, real_label) + criterion(fake_output, fake_label)) * 0.5\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        optimizer_G.zero_grad()\n",
        "        fake_output = discriminator(gray, fake_color)\n",
        "\n",
        "        # Generator loss\n",
        "        g_loss = criterion(fake_output, real_label) + l1_loss(fake_color, color) * 100\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # === Save sample output ===\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        save_image(fake_color, f\"/content/fake_epoch_{epoch+1}.png\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2AJrR5Beh2v",
        "outputId": "e776be2a-61c2-44b2-9d0b-e8880a38fecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "✅ Matched 76 grayscale-color pairs.\n",
            "Epoch [1/100] - D Loss: 0.6136 | G Loss: 26.7963\n",
            "Epoch [2/100] - D Loss: 0.4821 | G Loss: 20.6056\n",
            "Epoch [3/100] - D Loss: 0.3552 | G Loss: 16.2678\n",
            "Epoch [4/100] - D Loss: 0.3279 | G Loss: 15.8901\n"
          ]
        }
      ]
    }
  ]
}